{"cells":[{"outputs":[],"execution_count":1,"source":"# 查看当前挂载的数据集目录\n\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4C9D396F5E444AD48439990F8D733773","scrolled":false}},{"outputs":[],"execution_count":2,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"80EB0F8D56AC49E180C23A60B999950F","scrolled":false}},{"outputs":[],"execution_count":3,"source":"# 查看当前kernel下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3A4B2FDE4D374827802695A778F58CC3","scrolled":false}},{"outputs":[],"execution_count":4,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0689F9388A9B41078CDFD0314B6BDFE5","scrolled":false}},{"metadata":{"id":"311546F0BF004FF289B6869EE5AF0489","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple transformers==2.8.0","execution_count":5},{"metadata":{"id":"A735D28C43564E3F88173D6F5B276C5E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 1.导入运行所需包"},{"metadata":{"id":"AC068E12DE26484A8B04C54E9B5A3B32","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"from tqdm import tqdm, trange\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport logging\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport torch\nimport random\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, Subset, DataLoader\nimport os\n\nfrom transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig,get_linear_schedule_with_warmup,XLNetModel,XLNetTokenizer,XLNetConfig\nfrom transformers.optimization import get_linear_schedule_with_warmup\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, f1_score\nfrom sklearn.utils import shuffle\n","execution_count":1},{"metadata":{"id":"6CEB56E7B48041C189978CC9FF5C85CA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 2.超参数配置\n其中semi为模型预测的测试集结果(采用相同模型相同参数得出，区别为训练集为原始训练集，在测试集得分为0.66411980)，采用半监督方式进行再训练"},{"metadata":{"id":"E3327E9D1A574DE28DB401735A83A290","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"\n\ndevice = torch.device('cuda')\n\n\n#model_path = '../chinese_wwm_pytorch/'\n#model_path = '../chinese_xlnet_mid_pytorch/'\n#model_path = \"../MC-BERT/\"\nmodel_path = \"/home/kesci/input/model2308/chinese_roberta_wwm_large_ext_pytorch/\"\n\n\nbert_config = BertConfig.from_pretrained(model_path + 'bert_config.json', output_hidden_states=True)\ntokenizer = BertTokenizer.from_pretrained(model_path + 'vocab.txt', config=bert_config)\n\n\nseed = 2020\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n\nMAX_LEN = 512\nepoch = 5\nnum_class = 6\nlearn_rate = 2e-5\ntrain_batch_size = 4\nvalid_batch_size = 4\n\n\nfile_path = './log/'\n# 创建一个logger\nlogger = logging.getLogger('mylogger')\nlogger.setLevel(logging.DEBUG)\n\ntrain = pd.read_csv('/home/kesci/input/data5871/train.csv')\nsemi = pd.read_csv('/home/kesci/input/data5871/Semi-supervised_test.csv')\ntrain=pd.concat([train,semi],sort=False)\ntest = pd.read_csv('/home/kesci/input/data5871/nlp_test.csv')\nsub = pd.read_csv('/home/kesci/input/data5871/sample_submission.csv')\n\n\n\ntrain_content = train['Question Sentence'].values.astype(str)\ntest_content = test['Question Sentence'].values.astype(str)\n\n\ncategory_A = train['category_A'].astype(int).values\ncategory_B = train['category_B'].astype(int).values\ncategory_C = train['category_C'].astype(int).values\ncategory_D = train['category_D'].astype(int).values\ncategory_E = train['category_E'].astype(int).values\ncategory_F = train['category_F'].astype(int).values\n\ntest_category_A = [0] * len(test)\ntest_category_B = [0] * len(test)\ntest_category_C = [0] * len(test)\ntest_category_D = [0] * len(test)\ntest_category_E = [0] * len(test)\ntest_category_F = [0] * len(test)\n\n\ntrain_label = np.column_stack((category_A, category_B, category_C,category_D,category_E,category_F))\ntest_label = np.column_stack((test_category_A, test_category_B, test_category_C,test_category_D,test_category_E,test_category_F))\n\noof_train = np.zeros((len(train), num_class), dtype=np.float32)\noof_test = np.zeros((len(test), num_class), dtype=np.float32)","execution_count":9},{"metadata":{"id":"58D5CEA1166B46BC89BEE29CCC8F8ACB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 3.模型定义"},{"metadata":{"id":"313526C5C2B243FD8FC7B165EE16C087","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class BertForClass(nn.Module):\n    def __init__(self, n_classes=num_class):\n        super(BertForClass, self).__init__()\n        self.model_name = 'BertForClass'\n        self.bert_model = BertModel.from_pretrained(model_path, config=bert_config)\n        self.dropout = nn.Dropout(p=0.2)\n        self.multi_drop = 5\n        self.multi_dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(self.multi_drop)])\n        self.classifier = nn.Linear(bert_config.hidden_size * 2, n_classes)\n\n    def forward(self, input_ids, input_masks, segment_ids):\n        sequence_output, pooler_output, hidden_states = self.bert_model(input_ids=input_ids, token_type_ids=segment_ids,\n                                                                        attention_mask=input_masks)\n        seq_avg = torch.mean(sequence_output, dim=1)\n        concat_out = torch.cat((seq_avg, pooler_output), dim=1)\n        logit = self.classifier(self.dropout(concat_out))\n\n        '''\n        for j,dropout in enumerate(self.multi_dropouts):\n            if j == 0:\n                logit = self.classifier(dropout(concat_out)) / self.multi_drop\n            else:\n                logit += self.classifier(dropout(concat_out)) / self.multi_drop\n        '''\n        \n        return logit\n","execution_count":10},{"metadata":{"id":"83C2781FFB4C469888E38DCFF25CD2C8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 4.数据格式定义\n采用长度512文本，取前255+后255结合的方式"},{"metadata":{"id":"83FFDF0F050845F9A2CE9DA66ED6C9E8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class data_generator:\n    def __init__(self, data, batch_size=16, max_length=MAX_LEN, shuffle=False):\n        self.data = data\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.shuffle = shuffle\n        self.steps = len(self.data[0]) // self.batch_size\n        if len(self.data[0]) % self.batch_size != 0:\n            self.steps += 1\n\n    def __len__(self):\n        return self.steps\n\n    def __iter__(self):\n        c, y = self.data\n        idxs = list(range(len(self.data[0])))\n        if self.shuffle:\n            np.random.shuffle(idxs)\n        input_ids, input_masks, segment_ids, labels = [], [], [], []\n\n        for index, i in enumerate(idxs):\n\n            text = c[i]\n            if len(text) > 512:\n              text = text[:255] + text[-255:-1]\n\n            input_id = tokenizer.encode(text, max_length=self.max_length)\n            input_mask = [1] * len(input_id)\n            segment_id = [0] * len(input_id)\n            padding_length = self.max_length - len(input_id)\n            input_id += ([0] * padding_length)\n            input_mask += ([0] * padding_length)\n            segment_id += ([0] * padding_length)\n\n            input_ids.append(input_id)\n            input_masks.append(input_mask)\n            segment_ids.append(segment_id)\n            labels.append(y[i])\n            if len(input_ids) == self.batch_size or i == idxs[-1]:\n                yield input_ids, input_masks, segment_ids, labels\n                input_ids, input_masks, segment_ids, labels = [], [], [], []\n\n\n\n","execution_count":11},{"metadata":{"id":"07A94010528945E1A390EAE9A862D19F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 5.采用5折交叉验证的方式训练"},{"metadata":{"id":"F736BE72DBB845E0883C143A2FCFBEA6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train_content, train_label)):\n    print('\\n\\n------------fold:{}------------\\n'.format(fold))\n    c = train_content[train_index]\n    y = train_label[train_index]\n\n    val_c = train_content[valid_index]\n    val_y = train_label[valid_index]\n\n    train_D = data_generator([c, y], batch_size=train_batch_size, shuffle=True)\n    val_D = data_generator([val_c, val_y], batch_size=valid_batch_size)\n\n    model = BertForClass().to(device)\n    #pgd = PGD(model)\n    #K = 3\n    loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss就是把Sigmoid-BCELoss合成一步\n    \n    num_train_steps = int(len(train) / train_batch_size * epoch)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=learn_rate)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=int(len(train) / train_batch_size/2), \n        num_training_steps=num_train_steps\n    )\n\n    best_f1 = 0\n    PATH = '/home/kesci/work/bert_{}.pth'.format(fold)\n    for e in range(epoch):\n        print('\\n------------epoch:{}------------'.format(e))\n        model.train()\n        f1 = 0\n        train_len = 0\n        loss_num = 0\n        tq = tqdm(train_D)\n\n        for input_ids, input_masks, segment_ids, labels in tq:\n            input_ids = torch.tensor(input_ids).to(device)\n            input_masks = torch.tensor(input_masks).to(device)\n            segment_ids = torch.tensor(segment_ids).to(device)\n            label_t = torch.tensor(labels, dtype=torch.long).to(device)\n\n            y_pred = model(input_ids, input_masks, segment_ids)\n\n            loss = loss_fn(y_pred, label_t.float())\n            loss.backward()\n            '''\n            pgd.backup_grad()\n            # 对抗训练\n            for t in range(K):\n                pgd.attack(is_first_attack=(t == 0))  # 在embedding上添加对抗扰动, first attack时备份param.data\n                if t != K - 1:\n                    model.zero_grad()\n                else:\n                    pgd.restore_grad()\n                y_pred = model(input_ids, input_masks, segment_ids)\n\n                loss_adv = loss_fn(y_pred, label_t.float())\n                loss_adv.backward()  # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n            pgd.restore()  # 恢复embedding参数\n            '''\n\n            # 梯度下降，更新参数\n            optimizer.step()\n            scheduler.step()  # Update learning rate schedule\n            model.zero_grad()\n\n            y_pred = y_pred.sigmoid()\n            y_pred = y_pred.detach().to(\"cpu\").numpy()\n            res = []\n            for i in range(len(y_pred)):\n                tmp = []\n                for j in range(len(y_pred[0])):\n                    if y_pred[i][j] > 0.5:\n                        tmp.append(1)\n                    else:\n                        tmp.append(0)\n                res.append(tmp)\n\n            f1 += f1_score(np.array(res),np.array(labels),average = \"macro\")\n            loss_num += loss.item()\n            train_len += len(labels)\n            tq.set_postfix(fold=fold, epoch=e, loss=loss_num / train_len, f1=f1 / train_len)\n\n\n        model.eval()\n        with torch.no_grad():\n          y_p = []\n          train_logit = None\n          for input_ids, input_masks, segment_ids, labels in tqdm(val_D):\n              input_ids = torch.tensor(input_ids).to(device)\n              input_masks = torch.tensor(input_masks).to(device)\n              segment_ids = torch.tensor(segment_ids).to(device)\n              label_t = torch.tensor(labels, dtype=torch.long).to(device)\n\n              y_pred = model(input_ids, input_masks, segment_ids)\n\n              y_pred = y_pred.sigmoid()\n              y_pred = y_pred.detach().to(\"cpu\").numpy()\n\n              if train_logit is None:\n                  train_logit = y_pred\n              else:\n                  train_logit = np.vstack((train_logit, y_pred))\n              for i in range(len(y_pred)):\n                  tmp = []\n                  for j in range(len(y_pred[0])):\n                      if y_pred[i][j] > 0.5:\n                          tmp.append(1)\n                      else:\n                          tmp.append(0)\n                  y_p.append(tmp)\n\n          f1 = f1_score(np.array(y_p),np.array(val_y),average = \"macro\")\n          print(\"best_f1:{}  f1:{}\\n\".format(best_f1, f1))\n          if f1 >= best_f1:\n              best_f1 = f1\n              oof_train[valid_index] = np.array(train_logit)\n              torch.save(model, PATH)\n\n\n    test_D = data_generator([test_content, test_label], batch_size=valid_batch_size)\n    model = torch.load(PATH).to(device)\n    model.eval()\n    with torch.no_grad():\n      res = []\n      pred_logit = None\n\n      for input_ids, input_masks, segment_ids, labels in tqdm(test_D):\n          input_ids = torch.tensor(input_ids).to(device)\n          input_masks = torch.tensor(input_masks).to(device)\n          segment_ids = torch.tensor(segment_ids).to(device)\n\n          y_pred = model(input_ids, input_masks, segment_ids)\n          y_pred = y_pred.sigmoid()\n          y_pred = y_pred.detach().to(\"cpu\").numpy()\n\n          if pred_logit is None:\n              pred_logit = y_pred\n          else:\n              pred_logit = np.vstack((pred_logit, y_pred))\n\n    \n    oof_test += np.array(pred_logit)\n\n    optimizer.zero_grad()\n\n    del model\n    torch.cuda.empty_cache()\n\n\n%load_ext klab-autotime","execution_count":12},{"metadata":{"id":"8C90DF0EF109496F87A92C256D79211E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"# 6.导出结果"},{"metadata":{"id":"2F1D62A70DBC49FAAD944F7A79950284","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"oof_test /= 5\nres = []\nfor i in range(len(oof_test)):\n    tmp = []\n    for j in range(len(oof_test[0])):\n        if oof_test[i][j] > 0.5:\n            tmp.append(1)\n        else:\n            tmp.append(0)\n    res.append(tmp)\n\n\nsave_result_path = '/home/kesci/work/'\nif not os.path.exists(save_result_path):\n  os.makedirs(save_result_path)\n\nres = np.array(res)\nsub[\"category_A\"],sub[\"category_B\"],sub[\"category_C\"],sub[\"category_D\"],sub[\"category_E\"],sub[\"category_F\"] = res[:, 0],res[:, 1],res[:, 2],res[:, 3],res[:, 4],res[:, 5]\nsub.to_csv(\"/home/kesci/work/result.csv\",index=False)\n","execution_count":13},{"metadata":{"id":"15627CE20C174E348222935852F1E46C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"!wget -nv -O kesci_submit https://cdn.kesci.com/submit_tool/v4/kesci_submit&&chmod +x kesci_submit","execution_count":1},{"metadata":{"id":"E1514654E14D4E408DFADCF953135AD5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"!./kesci_submit -token 9723d983ffd8620e -file /home/kesci/work/result.csv","execution_count":2},{"metadata":{"id":"698A6E3CEB344FC08DF7B16B618D06D2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"8BC0CF91EB1A4D7F843C7173B7D3A37F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}